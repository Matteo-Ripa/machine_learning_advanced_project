{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b64046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from numpy import asarray\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e90c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'/home/gusripama@GU.GU.SE/machine_learning_2/project/CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdcfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.models as models\n",
    "import torchmetrics\n",
    "from my_resnet50 import my_ResNet50\n",
    "import tqdm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "import time\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ce89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these directory path in base of where the data json and images are\n",
    "# dir_data_json= r'/home/matteo/Desktop/GU/2nd_year/machine_learning_advanced/project_course/CODE/data'\n",
    "# dir_img= r'/home/matteo/Desktop/GU/2nd_year/machine_learning_advanced/project_course/CODE/images'\n",
    "dir_data_json= r'/home/gusripama@GU.GU.SE/machine_learning_2/project/CODE/data'\n",
    "dir_img= r'/home/gusripama@GU.GU.SE/machine_learning_2/project/CODE/images'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729cc4c",
   "metadata": {},
   "source": [
    "## **1. Dataset generation**\n",
    "\n",
    "The dataset creation happens through the file dataset_creation.py ; run it and it will create the dataset needed to work with!\n",
    "\n",
    "Here the list of probability distirbutions images that will be created:\n",
    "\n",
    "- Beta, Chi-squared, Exponential, Gamma, Laplace, Normal, Uniform, Weibul\n",
    "\n",
    "The size of the dataset is about to be: ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc8720",
   "metadata": {},
   "source": [
    "## **2. Load Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6fc83",
   "metadata": {},
   "source": [
    "#### **2.1 Open file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b29f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the info.json file\n",
    "os.chdir(dir_data_json)\n",
    "\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd4058",
   "metadata": {},
   "source": [
    "#### **2.2 Data Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e86e4",
   "metadata": {},
   "source": [
    "##### 2.2.1 Mapping distribution names with integers labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdd222",
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_list= []\n",
    "for key, val in data.items():\n",
    "    distr_list.append(val['label'])\n",
    "# Keep unique values\n",
    "distr_list= list(set(distr_list))\n",
    "\n",
    "# Map values wiht integers\n",
    "distr_to_int= {}\n",
    "int_to_distr= {}\n",
    "for i in range(len(distr_list)):\n",
    "    distr_to_int[distr_list[i]]= i\n",
    "    int_to_distr[i]= distr_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414ebde",
   "metadata": {},
   "source": [
    "##### 2.2.2 Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef37d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data= [('img_0.png', 1), ('img_1.png', 4), ...]\n",
    "my_data= {}\n",
    "for name_img, vals in data.items():\n",
    "    distr_cat= distr_to_int[vals['label']]\n",
    "    my_data[name_img]= distr_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf76e95",
   "metadata": {},
   "source": [
    "#### **2.3 Dataset creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470689c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory where the data(images) are\n",
    "os.chdir(dir_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef443ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to transform the image to tensor and crop it\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c288116",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dt= []\n",
    "idx=0\n",
    "\n",
    "for name_file in os.listdir(dir_img):\n",
    "    if idx%100 == 0:\n",
    "        print(idx)\n",
    "    idx+=1\n",
    "\n",
    "    # regular matrix of the image\n",
    "    img = Image.open(name_file).convert('RGB')\n",
    "    np_img = transform(img)\n",
    "\n",
    "    img_distr_cat= my_data[name_file]\n",
    "\n",
    "    my_dt.append((np_img, img_distr_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32579f28",
   "metadata": {},
   "source": [
    "## **3. Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974ae6c",
   "metadata": {},
   "source": [
    "#### **3.1 Preparing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394aa05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_number_classes= len(distr_list)\n",
    "\n",
    "# Baseline hyperparameters\n",
    "batch_size= 4\n",
    "epochs= 10\n",
    "\n",
    "# train 80 - validation 10 - test 10\n",
    "s_80= int(80*len(my_dt)/100)\n",
    "s_90= s_80 + int(10*len(my_dt)/100)\n",
    "\n",
    "my_train= my_dt[:s_80]\n",
    "my_val= my_dt[s_80:s_90]\n",
    "my_test= my_dt[s_90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82986fc5",
   "metadata": {},
   "source": [
    "#### **3.2 Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3cb15",
   "metadata": {},
   "source": [
    "##### **3.2.1 Setting criterion and device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47980691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Models names to save the model after training\n",
    "models_name= ['resnet50_scratch_baseline.pth', 'resnet50_scratch_preTrained.pth']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c48127",
   "metadata": {},
   "source": [
    "##### **Scratch ResNet 50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_rn50_scratch= my_ResNet50(img_channels=3, num_classes= dt_number_classes)\n",
    "\n",
    "optimizer_scratch = optim.SGD(model_rn50_scratch.parameters(), lr=0.001, momentum=0.9)\n",
    "model_rn50_scratch = model_rn50_scratch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd65e4",
   "metadata": {},
   "source": [
    "##### **Pre-trained ResNet 50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b157dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace last layer with number of classes I have\n",
    "model_preTrain = resnet50(pretrained=True)\n",
    "\n",
    "# Replacing the last layer to match number of classes\n",
    "num_features = model_preTrain.fc.in_features\n",
    "model_preTrain.fc = nn.Linear(num_features, dt_number_classes)\n",
    "\n",
    "optimizer_preTrain = optim.SGD(model_preTrain.parameters(), lr=0.001, momentum=0.9)\n",
    "model_preTrain = model_preTrain.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c29d4f",
   "metadata": {},
   "source": [
    "## **4. Training and Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe4442",
   "metadata": {},
   "source": [
    "#### **4.1 Training and Test**\n",
    "\n",
    "For the baseline model we use only 500 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, dt_train, dt_test, criterion, optimizer, device, epochs, batch_size, number_of_classes):\n",
    "\n",
    "    df_results= {'Epoch': [], \n",
    "                 'Train Loss': [],\n",
    "                 'Test Loss': [],\n",
    "                 'Test Accuracy': [],\n",
    "                 'Test Recall': [],\n",
    "                 'Test Precision': [],\n",
    "                 'Test F1': [],\n",
    "                 'Labels_Preds': []}\n",
    "\n",
    "    dataloader_train = DataLoader(dt_train[:500], batch_size=batch_size, shuffle=True)\n",
    "    dataloader_test = DataLoader(dt_test[:500], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch: {epoch+1}')\n",
    "        # Train the model on the training set\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (inputs, labels) in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "            # Move the data to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the training loss\n",
    "            train_loss += loss.item() #* inputs.size(0) # inputs.size(0) is the numbe rof samples in the batch\n",
    "\n",
    "\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "\n",
    "        # Save pair of labels and predictions in case to observe better what's gone wrong for which classes\n",
    "        labels_preds= []\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(tqdm.tqdm(dataloader_test)):\n",
    "                # Move the data to the device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Predictions\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # Update the test loss and accuracy\n",
    "                test_loss += loss.item() #* inputs.size(0)\n",
    "                test_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "                # save labels and rpedictions\n",
    "                labels_preds.append(preds.tolist())\n",
    "\n",
    "\n",
    "\n",
    "        # training/test loss and accuracy normalization based on the lenght of the data\n",
    "        train_loss = train_loss / len(dataloader_train) #*batch_size\n",
    "        test_loss = test_loss / len(dataloader_test) #*batch_size\n",
    "        test_acc = (test_acc.double() / (len(dataloader_test)*batch_size)).item()\n",
    "        \n",
    "        df_results['Epoch'].append(epoch+1)\n",
    "        df_results['Train Loss'].append(round(train_loss, 4))\n",
    "        df_results['Test Loss'].append(round(test_loss, 4))\n",
    "        df_results['Test Accuracy'].append(round(test_acc, 4))\n",
    "        df_results['Labels_Preds'].append(labels_preds)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_scratch_ResNet= train_test_model(model_rn50_scratch, my_train, my_test, criterion, optimizer_scratch, device, \n",
    "                                       epochs=10, batch_size=4, number_of_classes= dt_number_classes)\n",
    "\n",
    "result_preTrain_ResNet= train_test_model(model_preTrain, my_train, my_test, criterion, optimizer_preTrain, device, \n",
    "                                         epochs=10, batch_size=4, number_of_classes= dt_number_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf861f1",
   "metadata": {},
   "source": [
    "#### **4.2 Visual Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85066bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(result_scratch_ResNet['Train Loss'], label='Scratch ResNet50')\n",
    "plt.plot(result_preTrain_ResNet['Train Loss'], label='Pre-trained ResNet50')\n",
    "plt.ylim(0,3)\n",
    "plt.legend()\n",
    "plt.title('Train Loss')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(result_scratch_ResNet['Test Loss'], label='Scratch ResNet50')\n",
    "plt.plot(result_preTrain_ResNet['Test Loss'], label='Pre-trained ResNet50')\n",
    "plt.ylim(0,3)\n",
    "plt.legend()\n",
    "plt.title('Test Loss')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(result_scratch_ResNet['Test Accuracy'], label='Scratch ResNet50')\n",
    "plt.plot(result_preTrain_ResNet['Test Accuracy'], label='Pre-trained ResNet50')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391eec8",
   "metadata": {},
   "source": [
    "## **5. Fine-tuning hyperparameters**\n",
    "\n",
    "Due to computational cost, only 100 images have been"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b708d",
   "metadata": {},
   "source": [
    "#### **5.1 Fine-tuning function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_finetune(epochs, batch_size, model, optimizer, dt_train, dt_val):\n",
    "\n",
    "    dataloader_train = DataLoader(dt_train[:500], batch_size=batch_size, shuffle=True)\n",
    "    dataloader_val = DataLoader(dt_val[:500], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "            # Move the data to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the training loss\n",
    "            train_loss += loss.item() #* inputs.size(0)\n",
    "\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(tqdm.tqdm(dataloader_val)):\n",
    "                # Move the data to the device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Predictions\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # Update the test loss and accuracy\n",
    "                val_loss += loss.item() #* inputs.size(0)\n",
    "                val_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "        # training/test loss and accuracy normalization based on the lenght of the data\n",
    "        train_loss = train_loss / len(dataloader_train) #*batch_size\n",
    "        val_loss = val_loss / len(dataloader_val) #*batch_size\n",
    "        val_acc = (val_acc.double() / (len(dataloader_val)*batch_size)).item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Store the results\n",
    "    result_dic = {\n",
    "        'params': (batch_size, epochs),\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_acc,\n",
    "        'epochs_trained': epoch + 1\n",
    "    }\n",
    "    \n",
    "    return result_dic\n",
    "\n",
    "def best_score_update(results, best_score, params, best_params):\n",
    "    print(results)\n",
    "    test_acc= results['val_accuracy']\n",
    "\n",
    "    if test_acc > best_score:\n",
    "        best_score = test_acc\n",
    "        best_params = params\n",
    "    \n",
    "    return best_score, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e18a31",
   "metadata": {},
   "source": [
    "#### **5.2 Fine-tuning process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_param_grid= {'batch_size': [4, 8, 16],\n",
    "             'epochs': [10, 20, 50]}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results_scratch, results_pretrain = [], []\n",
    "best_score_scratch, best_score_pretrain = 0, 0\n",
    "best_params_scratch, best_params_pretrain = None, None\n",
    "\n",
    "for params in ParameterGrid(my_param_grid):\n",
    "    print(f\"\\nTesting combination: {params}\")\n",
    "\n",
    "    model_scratch= my_ResNet50(img_channels=3, num_classes= dt_number_classes)\n",
    "    optimizer_scratch = optim.SGD(model_scratch.parameters(), lr=0.001, momentum=0.9)\n",
    "    model_scratch = model_scratch.to(device)\n",
    "\n",
    "    model_preTrain = resnet50(pretrained=True)\n",
    "    num_features = model_preTrain.fc.in_features\n",
    "    model_preTrain.fc = nn.Linear(num_features, dt_number_classes)\n",
    "    optimizer_preTrain = optim.SGD(model_preTrain.parameters(), lr=0.001, momentum=0.9)\n",
    "    model_preTrain = model_preTrain.to(device)\n",
    "\n",
    "    result_dic_scratch= hyperparam_finetune(epochs= params['epochs'], batch_size= params['batch_size'],\n",
    "                                         model= model_scratch, optimizer= optimizer_scratch, \n",
    "                                         dt_train= my_train, dt_val= my_val)\n",
    "    \n",
    "    result_dic_pretrain= hyperparam_finetune(epochs=  params['epochs'], batch_size= params['batch_size'],\n",
    "                                          model= model_preTrain, optimizer= optimizer_preTrain,\n",
    "                                          dt_train= my_train, dt_val= my_val)\n",
    "\n",
    "    results_scratch.append(result_dic_scratch)\n",
    "    results_pretrain.append(result_dic_pretrain)\n",
    "\n",
    "    best_score_scratch, best_params_scratch= best_score_update(result_dic_scratch, best_score_scratch, params, best_params_scratch)\n",
    "    best_score_pretrain, best_params_pretrain= best_score_update(result_dic_pretrain, best_score_pretrain, params, best_params_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19059204",
   "metadata": {},
   "source": [
    "## **6. Train best models**\n",
    "\n",
    "Train the best models with different size of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324898c3",
   "metadata": {},
   "source": [
    "#### **6.1 Define best hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1531be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_best_scratch, epoch_best_scratch= best_params_scratch['batch_size'], best_params_scratch['epochs']\n",
    "batch_size_best_preTrain, epoch_best_preTrain= best_params_pretrain['batch_size'], best_params_pretrain['epochs']\n",
    "dataset_size= [100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a08a48",
   "metadata": {},
   "source": [
    "#### **6.2 Functions to compute computational complexity: time, number of operations (FLOPs), and model size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time_train_test(model, dt_train, dt_test, criterion, optimizer, device,\n",
    "                            epochs, batch_size, number_of_classes):\n",
    "    start = time.time()\n",
    "    results = train_test_model(model, dt_train, dt_test, criterion, optimizer,\n",
    "                               device, epochs, batch_size, number_of_classes)\n",
    "    end = time.time()\n",
    "    elapsed = end - start  # seconds\n",
    "    return results, elapsed\n",
    "\n",
    "def measure_flops(model, input_size, device):\n",
    "    \"\"\"\n",
    "    input_size: tuple (1, 3, H, W) for a single input\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(*input_size).to(device)\n",
    "    macs, params = profile(model, inputs=(dummy_input,), verbose=False)  # MACs and params\n",
    "    return macs, params\n",
    "\n",
    "def measure_model_size_mb(model, temp_path=\"temp_model.pth\"):\n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    size_mb = os.path.getsize(temp_path) / (1024 * 1024)\n",
    "    os.remove(temp_path)\n",
    "    return size_mb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffaba8",
   "metadata": {},
   "source": [
    "#### **6.3 Training and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results_by_dt_size_scratch= {}\n",
    "results_by_dt_size_preTrain= {}\n",
    "\n",
    "input_size= (1, 3, 224, 224)\n",
    "\n",
    "\n",
    "for dt_size in dataset_size:\n",
    "\n",
    "    model_rn50_scratch= my_ResNet50(img_channels=3, num_classes= dt_number_classes)\n",
    "    optimizer_scratch = optim.SGD(model_rn50_scratch.parameters(), lr=0.001, momentum=0.9)\n",
    "    model_rn50_scratch = model_rn50_scratch.to(device)\n",
    "\n",
    "    model_preTrain = resnet50(pretrained=True)\n",
    "    num_features = model_preTrain.fc.in_features\n",
    "    model_preTrain.fc = nn.Linear(num_features, dt_number_classes)\n",
    "    optimizer_preTrain = optim.SGD(model_preTrain.parameters(), lr=0.001, momentum=0.9)\n",
    "    model_preTrain = model_preTrain.to(device)\n",
    "        \n",
    "    dt_train_sized= my_train[:dt_size]\n",
    "    dt_test_sized= my_test[:dt_size]\n",
    "\n",
    "\n",
    "\n",
    "    scratch_results, scratch_time = measure_time_train_test(\n",
    "        model_rn50_scratch, dt_train_sized, dt_test_sized,\n",
    "        criterion, optimizer_scratch, device,\n",
    "        epochs=epoch_best_scratch, batch_size=batch_size_best_scratch,\n",
    "        number_of_classes=dt_number_classes)\n",
    "\n",
    "    scratch_macs, scratch_params = measure_flops(model_rn50_scratch, input_size, device)\n",
    "\n",
    "    scratch_size_mb = measure_model_size_mb(model_rn50_scratch)\n",
    "    \n",
    "    results_by_dt_size_scratch[f'{dt_size}'] = {\n",
    "        'results': scratch_results,\n",
    "        'time_sec': scratch_time,\n",
    "        'MACs_per_forward': scratch_macs,\n",
    "        'params_count': scratch_params,\n",
    "        'model_size_MB': scratch_size_mb,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    pretrain_results, pretrain_time = measure_time_train_test(\n",
    "        model_preTrain, dt_train_sized, dt_test_sized,\n",
    "        criterion, optimizer_preTrain, device,\n",
    "        epochs=epoch_best_preTrain, batch_size=batch_size_best_preTrain,\n",
    "        number_of_classes=dt_number_classes\n",
    "    )\n",
    "\n",
    "    pretrain_macs, pretrain_params = measure_flops(model_preTrain, input_size, device)\n",
    "    pretrain_size_mb = measure_model_size_mb(model_preTrain)\n",
    "\n",
    "    results_by_dt_size_preTrain[f'{dt_size}'] = {\n",
    "        'results': pretrain_results,\n",
    "        'time_sec': pretrain_time,\n",
    "        'MACs_per_forward': pretrain_macs,\n",
    "        'params_count': pretrain_params,\n",
    "        'model_size_MB': pretrain_size_mb,\n",
    "    }\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830235c",
   "metadata": {},
   "source": [
    "#### **6.4 Visual representation results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert keys to sorted integer sizes\n",
    "sizes = sorted(int(k) for k in results_by_dt_size_scratch.keys())\n",
    "\n",
    "# extract metrics\n",
    "def get_metric_from_results(results_dict, metric_key, epoch=-1):\n",
    "    \"\"\"\n",
    "    metric_key: 'time_sec', 'model_size_MB', 'MACs_per_forward', or one of the df_results keys\n",
    "                inside ['results'] such as 'Test Accuracy', 'Test Loss', 'Test F1', etc.\n",
    "    epoch: which epoch index to use (-1 = last epoch)\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for sz in sizes:\n",
    "        entry = results_dict[str(sz)]\n",
    "        if metric_key in ['time_sec', 'model_size_MB', 'MACs_per_forward', 'params_count']:\n",
    "            values.append(entry[metric_key])\n",
    "        else:\n",
    "            metric_list = entry['results'][metric_key]\n",
    "            values.append(metric_list[epoch])\n",
    "    return values\n",
    "\n",
    "\n",
    "scratch_acc = get_metric_from_results(results_by_dt_size_scratch, 'Test Accuracy')\n",
    "pretrain_acc = get_metric_from_results(results_by_dt_size_preTrain, 'Test Accuracy')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(sizes, scratch_acc, marker='o', label='Scratch ResNet50')\n",
    "plt.plot(sizes, pretrain_acc, marker='s', label='Pretrained ResNet50')\n",
    "plt.xscale('log') \n",
    "plt.xlabel('Dataset size')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Test accuracy vs dataset size')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92bd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_macs(results_dict, epochs, batch_size):\n",
    "    total_macs_list = []\n",
    "    for sz in sizes:\n",
    "        entry = results_dict[str(sz)]\n",
    "        macs_per_forward = entry['MACs_per_forward']\n",
    "        num_batches = math.ceil(sz / batch_size)\n",
    "        total_macs = macs_per_forward * 2 * num_batches * epochs  # forward + backward\n",
    "        total_macs_list.append(total_macs)\n",
    "    return total_macs_list\n",
    "\n",
    "# Use the best epochs and batch sizes found earlier\n",
    "scratch_total_macs = compute_total_macs(\n",
    "    results_by_dt_size_scratch,\n",
    "    epochs=epoch_best_scratch,\n",
    "    batch_size=batch_size_best_scratch\n",
    ")\n",
    "\n",
    "pretrain_total_macs = compute_total_macs(\n",
    "    results_by_dt_size_preTrain,\n",
    "    epochs=epoch_best_preTrain,\n",
    "    batch_size=batch_size_best_preTrain\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(sizes, scratch_total_macs, marker='o', label='Scratch ResNet50')\n",
    "plt.plot(sizes, pretrain_total_macs, marker='s', label='Pretrained ResNet50')\n",
    "plt.xscale('log')       \n",
    "plt.yscale('log')      \n",
    "plt.xlabel('Dataset size (number of samples)')\n",
    "plt.ylabel('Total MACs (approx. training compute)')\n",
    "plt.title('Approximate Total MACs vs Dataset Size')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1bde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time (training+testing) vs dataset size\n",
    "scratch_time = get_metric_from_results(results_by_dt_size_scratch, 'time_sec')\n",
    "pretrain_time = get_metric_from_results(results_by_dt_size_preTrain, 'time_sec')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(sizes, scratch_time, marker='o', label='Scratch ResNet50')\n",
    "plt.plot(sizes, pretrain_time, marker='s', label='Pretrained ResNet50')\n",
    "plt.xscale('log')  \n",
    "plt.xlabel('Dataset size')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Total train+test time vs dataset size')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
